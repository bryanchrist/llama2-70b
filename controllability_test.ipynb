{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "192a99e4-d1e2-41a1-a1d3-573e8d040a86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2953164b90f4762b849e30e31271317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import builtins\n",
    "#os.environ['TRANSFORMERS_CACHE'] = '/project/SDS/research/christ_research/Llama 2/mammoth/cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/brc4cb/llama/cache'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "#from adapter-transformers import AdapterType, AdapterConfig, load_adapter\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ[\"HF_REMOTES_OFFLINE\"] = \"1\"\n",
    "#from dotenv import load_dotenv\n",
    "# Load the environmental variables from the .env file\n",
    "#load_dotenv()\n",
    "\n",
    "#token= os.environ['huggingface_token']\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#login(token = token)\n",
    "\n",
    "# Redirect stdin to /dev/null\n",
    "sys.stdin = open(os.devnull)\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B\"   # Specify the path to the model\n",
    "#adapter_path = \"mathwell8b-kto/checkpoint-1250\"  # Specify the path to the adapter weights\n",
    "adapter_path = \"mathwell/llama3-8b-two_stage/checkpoint-250\"  # Specify the path to the adapter weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\") #, use_auth_token=True,)\n",
    "\n",
    "# Patch the built-in input function to return 'y' automatically\n",
    "def mock_input(prompt=None):\n",
    "    return 'y'\n",
    "\n",
    "# Patch the input function to use the mock_input function\n",
    "builtins.input = mock_input\n",
    "\n",
    "try:\n",
    "    # Attempt to load the model with trust_remote_code=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        load_in_4bit=True, \n",
    "       # max_memory=max_memory,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        #use_auth_token=True,\n",
    "        config=AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    )\n",
    "except EOFError:\n",
    "    # If an EOFError occurs, provide the expected input ('y')\n",
    "    pass\n",
    "\n",
    "# Restore stdin\n",
    "sys.stdin = sys.__stdin__\n",
    "\n",
    "# Set special tokens\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=64)\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "        \n",
    "if tokenizer._pad_token is None:\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "print('Adding special tokens.')\n",
    "tokenizer.add_special_tokens({\n",
    "        \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "        \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "#                 \"unk_token\": tokenizer.convert_ids_to_tokens(\n",
    "#                     model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id\n",
    "#                 ),\n",
    "})\n",
    "\n",
    "# Load the adapter weights\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3fd6e1c-90a4-475e-9c84-481b846be05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/evaluation_annotations.csv')\n",
    "df = df[df['good']==1]\n",
    "df['output'] = \"Question: \" + df['question'] + \"\\n\" + \"Solution:\\n\" + df['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac9e1dd-6185-4c09-bf6c-1129948df6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "addition = df.query(\"addition==1 and total_ops==1\")\n",
    "subtraction = df.query(\"subtraction==1 and total_ops==1\")\n",
    "multiplication = df.query(\"multiplication==1 and total_ops==1\")\n",
    "division = df.query(\"division==1 and total_ops==1\")\n",
    "fractions = df.query(\"fractions==1 and total_ops==1\")\n",
    "decimals = df.query(\"decimals==1 and total_ops==1\")\n",
    "multi_ops = df.query(\"total_ops>1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ef7e00e-4f91-4dc3-9e8f-0b680c03cde2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 79 123 78 0 0 528\n"
     ]
    }
   ],
   "source": [
    "print(len(addition), len(subtraction), len(multiplication), len(division), len(fractions), len(decimals), len(multi_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "004a7530-f319-42e9-b2aa-9f6d5bbd6b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topics = ['Superman', \"Batman\", \"Wonder Woman\", \"Barbie\", \"Power Rangers\", \"basketball\", \"soccer\", \"football\", \"volleyball\", 'field hockey',\\\n",
    "'Fortnite', 'Spiderman', \"Iron Man\", \"Captain America\", \"Captain Marvel\", \"Thor, the God of Thunder\", \"Ninja Turtles\", \"Black Panther\", \"Taylor Swift\", \"swimming\",\\\n",
    "\"Pok√©mon\", \"Super Mario\", \"Naruto\", \"unicorns\", \"Hello Kitty\", \"Minecraft\", \"lacrosse\", \"cheer leading\", \"LeBron James\", \"Steph Curry\", \"Patrick Mahomes\",\\\n",
    "\"Serena Williams\", \"dogs\", \"cats\", \"dinosaurs\", \"Harry Potter\", \"cars\", \"planes\", \"trains\", \"pizza\", \"cookies\", \"ice cream\", 'candy']\n",
    "def prompt(df, model, tokenizer, n_qs, operation, operator, topics):\n",
    "    responses = []\n",
    "    while len(responses)<n_qs:\n",
    "        topic = random.choice(topics)\n",
    "        final_prompt = f\"Write a grade school math {operation} word problem about {topic} and Python function with a commented out step-by-step solution to solve the word problem. The question you write should only require {operation} to solve, meaning the solution should rely only on use of the {operator} operator.\"\n",
    "        prompt = f\"Write a grade school math {operation} word problem and Python function with a commented out step-by-step solution to solve the word problem. The question you write should only require {operation} to solve, meaning the solution should rely only on use of the {operator} operator.\"\n",
    "        #final_prompt = f\"Write a grade school math {operation} word problem about {topic} and Python function with a commented out step-by-step solution to solve the word problem.\"\n",
    "        #prompt = f\"Write a grade school math {operation} word problem and Python function with a commented out step-by-step solution to solve the word problem.\"\n",
    "        questions = []\n",
    "        while len(questions)<8:\n",
    "            question = df['output'].iloc[random.randint(0,len(df)-1)]\n",
    "            if question not in questions:\n",
    "                questions.append(question)\n",
    "        formatted_prompt = []\n",
    "        for i in range(0,8):\n",
    "            formatted_prompt.append((f\"Below is an instruction that describes a task. \"\n",
    "                    f\"Write a response that appropriately completes the request.\\n\\n\"\n",
    "                    f\"### Instruction:\\n{prompt}\\n\\n### Response: {questions[i]}\"))\n",
    "        formatted_prompt.append(f\"Below is an instruction that describes a task. \"\n",
    "                    f\"Write a response that appropriately completes the request.\\n\\n\"\n",
    "                    f\"### Instruction:\\n{final_prompt}\\n\\n### Response: \")\n",
    "        formatted_prompt = \"\\n\".join(formatted_prompt)\n",
    "        inputs = tokenizer.encode(formatted_prompt, return_tensors=\"pt\")\n",
    "        attention_mask = torch.ones_like(inputs)\n",
    "        inputs = inputs.to('cuda')\n",
    "        output = model.generate(inputs=inputs, attention_mask=attention_mask, max_new_tokens = 200, do_sample = True)\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Split the generated text by the prompt to extract the newly generated part\n",
    "        generated_text_parts = generated_text.split(final_prompt)\n",
    "        newly_generated_text = generated_text_parts[-1].strip()\n",
    "        if \"\\nBel\" in newly_generated_text:\n",
    "            newly_generated_text = newly_generated_text.split(\"\\nBel\")[0]\n",
    "        if \"Question:\" in newly_generated_text:\n",
    "            responses.append(newly_generated_text)\n",
    "    return responses\n",
    "        # output_file = \"mathwell8b_kto_questions_few.txt\"  # Specify the path and filename for the output file\n",
    "        # with open(output_file, \"a\") as f:  # Open the file in append mode (\"a\")\n",
    "        #     f.write(f\"Topic: {topic} \" + newly_generated_text + \"\\n\")  # Append the newly generated text to the file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffed38-3e4f-495e-a21d-3b3ab629d6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "qs = prompt(addition, model, tokenizer, 30, 'addition', \"+\", topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c834a42-d0aa-443d-95f1-f44c6959656d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_len = len(qs)\n",
    "add = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \" - \" not in solution and \" * \" not in solution and \"/\" not in solution and \".\" not in solution:\n",
    "            add+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(add/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3065d7-73dc-46bf-ac65-3266c5102a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_len = len(qs)\n",
    "add = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \" + \" in solution:\n",
    "            add+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(add/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce79c308-a146-480f-b470-9af95eba50af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "qs = prompt(subtraction, model, tokenizer, 30, 'subtraction', \"-\", topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0597ea5d-30bb-4151-a6cd-f1ad68cd3099",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6551724137931034\n"
     ]
    }
   ],
   "source": [
    "q_len = len(qs)\n",
    "sub = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \" + \" not in solution and \" * \" not in solution and \"/\" not in solution and \".\" not in solution:\n",
    "            sub+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(sub/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5a271356-5eb4-4347-afc8-b12c54b4bc30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "q_len = len(qs)\n",
    "sub = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \" - \" in solution:\n",
    "            sub+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(sub/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "110873f0-69d4-4b86-a6b8-c75c2b1c49af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "qs = prompt(multiplication, model, tokenizer, 30, 'multiplication', \"*\", topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d22d53e4-fae3-4d54-8c44-277add46ef4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "q_len = len(qs)\n",
    "mult = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \" + \" not in solution and \" - \" not in solution and \"/\" not in solution and \".\" not in solution:\n",
    "            mult+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(mult/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7acec25-2d89-41d0-978d-3db9473b78cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "q_len = len(qs)\n",
    "mult = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \"*\" in solution:\n",
    "            mult+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(mult/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe65f5c5-20ab-4b1d-ab93-f502f1f1ca80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "qs = prompt(division, model, tokenizer, 30, 'division', \"/\", topics=topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d210e1ac-a271-4a46-9db4-ba3a690da487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "q_len = len(qs)\n",
    "div = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \" + \" not in solution and \" - \" not in solution and \" * \" not in solution and \".\" not in solution:\n",
    "            div+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(div/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec8baae2-d855-44ab-9e8c-1550f1a4021b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32142857142857145\n"
     ]
    }
   ],
   "source": [
    "q_len = len(qs)\n",
    "div = 0\n",
    "for i in range(0, len(qs)):\n",
    "    try:\n",
    "        solution = qs[i].split(\"Solution:\")[1]\n",
    "        if \"/\" in solution:\n",
    "            div+=1\n",
    "    except:\n",
    "        q_len-=1\n",
    "print(div/q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9c4b0-a72a-450a-89d4-889a7ae4ae73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(0.21428571428571427+0.6206896551724138+0.75+0.4827586206896552)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de99cb-18a7-4e9c-9fae-4bac1a3a8c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
