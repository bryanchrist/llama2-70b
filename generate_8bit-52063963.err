  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-i7cly5x8/peft_2475603759e04cb1a11e92a2d2a52108
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-i7cly5x8/accelerate_e3fa51824a1546adbb753b1551363a10
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<00:58,  4.17s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:07<00:45,  3.49s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:10<00:40,  3.36s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:13<00:36,  3.29s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:17<00:34,  3.41s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:21<00:32,  3.57s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:24<00:29,  3.64s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:28<00:24,  3.54s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:31<00:20,  3.35s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:34<00:17,  3.48s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:37<00:13,  3.33s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:41<00:10,  3.46s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:45<00:07,  3.57s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:49<00:03,  3.59s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:49<00:00,  2.57s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:49<00:00,  3.29s/it]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
slurmstepd: error: *** JOB 52063963 ON udc-an34-7 CANCELLED AT 2023-08-10T14:14:26 DUE TO TIME LIMIT ***
