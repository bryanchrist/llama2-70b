  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-rolcag16/peft_1a01e63d42814eb79a300a8ee46896ef
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-rolcag16/accelerate_745ca22629cc4a9f9c92b9feb4592e3b
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:23<05:26, 23.33s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:44<04:48, 22.19s/it]Loading checkpoint shards:  20%|██        | 3/15 [01:04<04:12, 21.02s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [01:25<03:50, 21.00s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [01:46<03:32, 21.22s/it]Loading checkpoint shards:  40%|████      | 6/15 [02:08<03:10, 21.20s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [02:29<02:50, 21.34s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [02:49<02:26, 20.86s/it]Loading checkpoint shards:  60%|██████    | 9/15 [03:09<02:04, 20.69s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [03:33<01:47, 21.59s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [03:54<01:26, 21.53s/it]Loading checkpoint shards:  80%|████████  | 12/15 [04:15<01:03, 21.21s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [04:36<00:42, 21.22s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [04:56<00:20, 20.95s/it]Loading checkpoint shards: 100%|██████████| 15/15 [04:58<00:00, 14.96s/it]Loading checkpoint shards: 100%|██████████| 15/15 [04:58<00:00, 19.87s/it]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
