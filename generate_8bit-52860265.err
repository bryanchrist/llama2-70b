  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-3xjl0i9n/peft_8a27284783574c38879ad5a01710d0b8
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-3xjl0i9n/accelerate_d6ed4e5ab0084e41b80058e6a5573671
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:36<08:26, 36.20s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [01:19<08:45, 40.44s/it]Loading checkpoint shards:  20%|██        | 3/15 [01:53<07:27, 37.32s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [02:28<06:40, 36.40s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [03:00<05:50, 35.04s/it]Loading checkpoint shards:  40%|████      | 6/15 [03:44<05:42, 38.04s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [04:19<04:54, 36.85s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [04:48<04:00, 34.41s/it]Loading checkpoint shards:  60%|██████    | 9/15 [05:19<03:21, 33.53s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [05:51<02:44, 32.90s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [06:31<02:20, 35.11s/it]Loading checkpoint shards:  80%|████████  | 12/15 [07:18<01:55, 38.62s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [07:47<01:11, 35.77s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [08:12<00:32, 32.59s/it]Loading checkpoint shards: 100%|██████████| 15/15 [08:14<00:00, 23.20s/it]Loading checkpoint shards: 100%|██████████| 15/15 [08:14<00:00, 32.94s/it]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
slurmstepd: error: *** JOB 52860265 ON udc-an34-7 CANCELLED AT 2023-09-03T14:26:09 DUE TO TIME LIMIT ***
