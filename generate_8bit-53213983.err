  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-z4mykr6l/peft_6750f41bf3284eb6aa31fa5108393002
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-z4mykr6l/accelerate_42a34224f5044dbcb4a5e9884b128dae
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:27<06:27, 27.70s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:51<05:27, 25.18s/it]Loading checkpoint shards:  20%|██        | 3/15 [01:13<04:46, 23.88s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [01:36<04:20, 23.70s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [02:03<04:08, 24.86s/it]Loading checkpoint shards:  40%|████      | 6/15 [02:28<03:41, 24.65s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [02:58<03:31, 26.39s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [03:35<03:30, 30.06s/it]Loading checkpoint shards:  60%|██████    | 9/15 [04:04<02:56, 29.49s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [04:49<02:52, 34.54s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [05:15<02:07, 31.77s/it]Loading checkpoint shards:  80%|████████  | 12/15 [05:44<01:33, 31.06s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [06:08<00:57, 28.71s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [06:36<00:28, 28.64s/it]Loading checkpoint shards: 100%|██████████| 15/15 [06:37<00:00, 20.22s/it]Loading checkpoint shards: 100%|██████████| 15/15 [06:37<00:00, 26.49s/it]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
slurmstepd: error: *** JOB 53213983 ON udc-an36-13 CANCELLED AT 2023-09-15T14:50:43 ***
