  Running command git clone --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-qf9k_py2/transformers_e994514f1a954dcab88d0d750dad94ec
  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-qf9k_py2/peft_44b26c78c2a04c3d8f0f2396b2357eed
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-qf9k_py2/accelerate_841d19b73c24453aae122e4515db56e8
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so'), PosixPath('/home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
Downloading (…)okenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 264kB/s]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 47.0MB/s]
Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 53.9MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 292kB/s]
Downloading (…)lve/main/config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 630/630 [00:00<00:00, 339kB/s]
Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]Downloading shards:  20%|██        | 3/15 [00:00<00:00, 26.95it/s]Downloading shards:  47%|████▋     | 7/15 [00:00<00:00, 31.35it/s]Downloading shards:  73%|███████▎  | 11/15 [00:00<00:00, 18.87it/s]Downloading shards:  93%|█████████▎| 14/15 [00:00<00:00, 19.66it/s]Downloading shards: 100%|██████████| 15/15 [00:00<00:00, 21.27it/s]
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:31<07:24, 31.76s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [01:01<06:39, 30.72s/it]Loading checkpoint shards:  20%|██        | 3/15 [01:29<05:52, 29.42s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [01:57<05:17, 28.87s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [02:25<04:45, 28.57s/it]Loading checkpoint shards:  40%|████      | 6/15 [02:51<04:10, 27.80s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [03:20<03:43, 27.94s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [03:47<03:13, 27.70s/it]Loading checkpoint shards:  60%|██████    | 9/15 [04:13<02:44, 27.34s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [04:42<02:19, 27.81s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [05:10<01:50, 27.63s/it]Loading checkpoint shards:  80%|████████  | 12/15 [05:37<01:22, 27.65s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [06:03<00:54, 27.23s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [06:29<00:26, 26.68s/it]Loading checkpoint shards: 100%|██████████| 15/15 [06:30<00:00, 18.99s/it]Loading checkpoint shards: 100%|██████████| 15/15 [06:30<00:00, 26.04s/it]
Downloading (…)neration_config.json:   0%|          | 0.00/197 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 197/197 [00:00<00:00, 240kB/s]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )
  warnings.warn(
Traceback (most recent call last):
  File "/gpfs/gpfs0/project/SDS/research/christ_research/Llama 2/llama2-70b/generate_not_finetuned.py", line 188, in <module>
    output = model.generate(inputs=inputs, attention_mask=attention_mask, max_new_tokens = 400, do_sample = True, top_p = .5)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/generation/utils.py", line 1588, in generate
    return self.sample(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/generation/utils.py", line 2642, in sample
    outputs = self(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 756, in forward
    outputs = self.model(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 644, in forward
    layer_outputs = decoder_layer(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 359, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 261, in forward
    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
RuntimeError: shape '[1, 1140, 64, 128]' is invalid for input of size 1167360
