Collecting transformers@ git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 2))
  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-8fp5yt2z/transformers_285488ca1350447c950970f87a7de417
  Resolved https://github.com/huggingface/transformers.git to commit f614b6e393e4c1ec4ae39be0cc36e1e1f6907740
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting peft@ git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 3))
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-install-8fp5yt2z/peft_886a5b6039d44d1b9743bf663504dd25
  Resolved https://github.com/huggingface/peft.git to commit 06fd06a4d2e8ed8c3a253c67d9c3cb23e0f497ad
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting accelerate@ git+https://github.com/huggingface/accelerate.git (from -r requirements.txt (line 4))
  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-install-8fp5yt2z/accelerate_0d833f6aa67e4d1c8552d9a15b8dac06
  Resolved https://github.com/huggingface/accelerate.git to commit 95bffdec4326acf6a5d1c3dbaa857a26502aa265
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: bitsandbytes==0.39.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.39.0)
Requirement already satisfied: einops==0.6.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.6.1)
Requirement already satisfied: evaluate==0.4.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.4.0)
Requirement already satisfied: scikit-learn==1.2.2 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.2.2)
Requirement already satisfied: scipy in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.11.1)
Requirement already satisfied: sentencepiece==0.1.99 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.1.99)
Requirement already satisfied: wandb==0.15.3 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.15.3)
Requirement already satisfied: datasets>=2.0.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2.13.1)
Requirement already satisfied: numpy>=1.17 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (1.25.0)
Requirement already satisfied: dill in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.3.6)
Requirement already satisfied: pandas in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2.0.2)
Requirement already satisfied: requests>=2.19.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (4.65.0)
Requirement already satisfied: xxhash in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (3.2.0)
Requirement already satisfied: multiprocess in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.70.14)
Requirement already satisfied: fsspec[http]>=2021.05.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2023.4.0)
Requirement already satisfied: huggingface-hub>=0.7.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.15.1)
Requirement already satisfied: packaging in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (23.1)
Requirement already satisfied: responses<0.19 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.18.0)
Requirement already satisfied: joblib>=1.1.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (3.1.0)
Requirement already satisfied: Click!=8.0.0,>=7.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (8.1.3)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (3.1.31)
Requirement already satisfied: psutil>=5.0.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (5.9.5)
Requirement already satisfied: sentry-sdk>=1.0.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (1.26.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (0.4.0)
Requirement already satisfied: PyYAML in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (6.0)
Requirement already satisfied: pathtools in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (0.1.2)
Requirement already satisfied: setproctitle in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (1.3.2)
Requirement already satisfied: setuptools in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (67.8.0)
Requirement already satisfied: appdirs>=1.4.3 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (1.4.4)
Requirement already satisfied: typing-extensions in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (4.4.0)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (4.23.3)
Requirement already satisfied: filelock in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 2)) (3.9.0)
Requirement already satisfied: regex!=2019.12.17 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 2)) (2023.6.3)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 2)) (0.13.3)
Requirement already satisfied: safetensors>=0.3.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->-r requirements.txt (line 2)) (0.3.1)
Requirement already satisfied: torch>=1.13.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (2.1.0.dev20230624+cu118)
Requirement already satisfied: pyarrow>=8.0.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (12.0.1)
Requirement already satisfied: aiohttp in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (3.8.4)
Requirement already satisfied: six>=1.4.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb==0.15.3->-r requirements.txt (line 10)) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 10)) (4.0.10)
Requirement already satisfied: charset-normalizer<4,>=2 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 6)) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 6)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 6)) (2.0.3)
Requirement already satisfied: certifi>=2017.4.17 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from requests>=2.19.0->evaluate==0.4.0->-r requirements.txt (line 6)) (2023.5.7)
Requirement already satisfied: sympy in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (1.11.1)
Requirement already satisfied: networkx in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (3.0rc1)
Requirement already satisfied: jinja2 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (3.1.2)
Requirement already satisfied: pytorch-triton==2.1.0+440fd1bf20 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from torch>=1.13.0->peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (2.1.0+440fd1bf20)
Requirement already satisfied: python-dateutil>=2.8.2 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 6)) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 6)) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 6)) (2023.3)
Requirement already satisfied: attrs>=17.3.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (1.3.1)
Requirement already satisfied: smmap<6,>=3.0.1 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 10)) (5.0.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (2.1.2)
Requirement already satisfied: mpmath>=0.19 in /sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft@ git+https://github.com/huggingface/peft.git->-r requirements.txt (line 3)) (1.2.1)
Collecting package metadata (current_repodata.json): ...working... done
Solving environment: ...working... done

# All requested packages already installed.


===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so
CUDA SETUP: CUDA runtime path found: /home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 118
CUDA SETUP: Loading binary /home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...
[2023-07-07 14:48:09,186] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
loading base model huggyllama/llama-65b...
adding LoRA modules...
trainable params: 399769600.0 || all params: 33705172992 || trainable: 1.1860778762206212
loaded model
Adding special tokens.
Downloading and preparing dataset json/default to /home/brc4cb/.cache/huggingface/datasets/json/default-c11a0d8e43cff720/0.0.0...
Dataset json downloaded and prepared to /home/brc4cb/.cache/huggingface/datasets/json/default-c11a0d8e43cff720/0.0.0. Subsequent calls will reuse this data.
Splitting train dataset in train and validation according to `eval_dataset_size`
Downloading and preparing dataset json/default to /home/brc4cb/.cache/huggingface/datasets/json/default-b13e847a739664c0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Dataset json downloaded and prepared to /home/brc4cb/.cache/huggingface/datasets/json/default-b13e847a739664c0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
torch.bfloat16 1323843584 0.039277144217520744
torch.uint8 32380026880 0.9606837249535352
torch.float32 1318912 3.913082894407767e-05
{'loss': 0.9043, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 0.848, 'learning_rate': 0.0001, 'epoch': 0.05}
{'loss': 0.8233, 'learning_rate': 0.0001, 'epoch': 0.07}
{'loss': 0.8319, 'learning_rate': 0.0001, 'epoch': 0.09}
{'loss': 0.796, 'learning_rate': 0.0001, 'epoch': 0.12}
{'loss': 0.7038, 'learning_rate': 0.0001, 'epoch': 0.14}
{'loss': 0.7363, 'learning_rate': 0.0001, 'epoch': 0.16}
{'loss': 0.751, 'learning_rate': 0.0001, 'epoch': 0.19}
{'loss': 0.7718, 'learning_rate': 0.0001, 'epoch': 0.21}
{'loss': 0.7912, 'learning_rate': 0.0001, 'epoch': 0.23}
{'loss': 0.732, 'learning_rate': 0.0001, 'epoch': 0.26}
{'loss': 0.7628, 'learning_rate': 0.0001, 'epoch': 0.28}
{'loss': 0.7783, 'learning_rate': 0.0001, 'epoch': 0.3}
{'loss': 0.7765, 'learning_rate': 0.0001, 'epoch': 0.33}
{'loss': 0.7751, 'learning_rate': 0.0001, 'epoch': 0.35}
{'loss': 0.6959, 'learning_rate': 0.0001, 'epoch': 0.37}
{'loss': 0.7557, 'learning_rate': 0.0001, 'epoch': 0.39}
{'loss': 0.745, 'learning_rate': 0.0001, 'epoch': 0.42}
{'eval_loss': 0.7376705408096313, 'eval_runtime': 819.26, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 0.43}
{'mmlu_loss': 2.2116388511533134, 'mmlu_eval_accuracy_econometrics': 0.5833333333333334, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.18181818181818182, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.75, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_sociology': 0.9545454545454546, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.5454545454545454, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_mathematics': 0.20689655172413793, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.5172413793103449, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_law': 0.4, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_moral_scenarios': 0.53, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.5, 'mmlu_eval_accuracy_electrical_engineering': 0.375, 'mmlu_eval_accuracy': 0.5983201343562762, 'epoch': 0.43}
{'loss': 0.8104, 'learning_rate': 0.0001, 'epoch': 0.44}
{'loss': 0.7563, 'learning_rate': 0.0001, 'epoch': 0.46}
Saving PEFT checkpoint...
{'loss': 0.692, 'learning_rate': 0.0001, 'epoch': 0.49}
{'loss': 0.7047, 'learning_rate': 0.0001, 'epoch': 0.51}
{'loss': 0.7603, 'learning_rate': 0.0001, 'epoch': 0.53}
{'loss': 0.7721, 'learning_rate': 0.0001, 'epoch': 0.56}
{'loss': 0.7489, 'learning_rate': 0.0001, 'epoch': 0.58}
{'loss': 0.6783, 'learning_rate': 0.0001, 'epoch': 0.6}
{'loss': 0.7314, 'learning_rate': 0.0001, 'epoch': 0.63}
{'loss': 0.7256, 'learning_rate': 0.0001, 'epoch': 0.65}
{'loss': 0.7646, 'learning_rate': 0.0001, 'epoch': 0.67}
{'loss': 0.7729, 'learning_rate': 0.0001, 'epoch': 0.7}
{'loss': 0.6592, 'learning_rate': 0.0001, 'epoch': 0.72}
{'loss': 0.7285, 'learning_rate': 0.0001, 'epoch': 0.74}
{'loss': 0.757, 'learning_rate': 0.0001, 'epoch': 0.77}
{'loss': 0.7582, 'learning_rate': 0.0001, 'epoch': 0.79}
{'loss': 0.74, 'learning_rate': 0.0001, 'epoch': 0.81}
{'loss': 0.6591, 'learning_rate': 0.0001, 'epoch': 0.84}
{'loss': 0.6664, 'learning_rate': 0.0001, 'epoch': 0.86}
{'eval_loss': 0.7214663028717041, 'eval_runtime': 818.82, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 0.87}
{'mmlu_loss': 2.56506000820287, 'mmlu_eval_accuracy_econometrics': 0.6666666666666666, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6363636363636364, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_sociology': 0.9545454545454546, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_college_medicine': 0.4090909090909091, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_global_facts': 0.5, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.42, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_statistics': 0.2608695652173913, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.4375, 'mmlu_eval_accuracy': 0.5961525884033017, 'epoch': 0.87}
{'loss': 0.7486, 'learning_rate': 0.0001, 'epoch': 0.88}
{'loss': 0.7568, 'learning_rate': 0.0001, 'epoch': 0.91}
{'loss': 0.7265, 'learning_rate': 0.0001, 'epoch': 0.93}
Saving PEFT checkpoint...
{'loss': 0.7005, 'learning_rate': 0.0001, 'epoch': 0.95}
{'loss': 0.736, 'learning_rate': 0.0001, 'epoch': 0.98}
{'loss': 0.7269, 'learning_rate': 0.0001, 'epoch': 1.0}
{'loss': 0.6076, 'learning_rate': 0.0001, 'epoch': 1.02}
{'loss': 0.5872, 'learning_rate': 0.0001, 'epoch': 1.05}
{'loss': 0.5951, 'learning_rate': 0.0001, 'epoch': 1.07}
{'loss': 0.6007, 'learning_rate': 0.0001, 'epoch': 1.09}
{'loss': 0.588, 'learning_rate': 0.0001, 'epoch': 1.11}
{'loss': 0.5454, 'learning_rate': 0.0001, 'epoch': 1.14}
{'loss': 0.5912, 'learning_rate': 0.0001, 'epoch': 1.16}
{'loss': 0.5772, 'learning_rate': 0.0001, 'epoch': 1.18}
{'loss': 0.6134, 'learning_rate': 0.0001, 'epoch': 1.21}
{'loss': 0.5888, 'learning_rate': 0.0001, 'epoch': 1.23}
{'loss': 0.5175, 'learning_rate': 0.0001, 'epoch': 1.25}
{'loss': 0.5875, 'learning_rate': 0.0001, 'epoch': 1.28}
{'loss': 0.6222, 'learning_rate': 0.0001, 'epoch': 1.3}
{'eval_loss': 0.7349767684936523, 'eval_runtime': 818.8571, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 1.3}
{'mmlu_loss': 2.967704406678482, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.6363636363636364, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7674418604651163, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.2857142857142857, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.5806451612903226, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.45, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy': 0.6019712321472391, 'epoch': 1.3}
{'loss': 0.5964, 'learning_rate': 0.0001, 'epoch': 1.32}
{'loss': 0.6143, 'learning_rate': 0.0001, 'epoch': 1.35}
{'loss': 0.5653, 'learning_rate': 0.0001, 'epoch': 1.37}
{'loss': 0.577, 'learning_rate': 0.0001, 'epoch': 1.39}
Saving PEFT checkpoint...
{'loss': 0.624, 'learning_rate': 0.0001, 'epoch': 1.42}
{'loss': 0.6032, 'learning_rate': 0.0001, 'epoch': 1.44}
{'loss': 0.5965, 'learning_rate': 0.0001, 'epoch': 1.46}
{'loss': 0.5876, 'learning_rate': 0.0001, 'epoch': 1.49}
{'loss': 0.6021, 'learning_rate': 0.0001, 'epoch': 1.51}
{'loss': 0.6183, 'learning_rate': 0.0001, 'epoch': 1.53}
{'loss': 0.615, 'learning_rate': 0.0001, 'epoch': 1.56}
{'loss': 0.6103, 'learning_rate': 0.0001, 'epoch': 1.58}
{'loss': 0.5583, 'learning_rate': 0.0001, 'epoch': 1.6}
{'loss': 0.5983, 'learning_rate': 0.0001, 'epoch': 1.63}
{'loss': 0.6214, 'learning_rate': 0.0001, 'epoch': 1.65}
{'loss': 0.6239, 'learning_rate': 0.0001, 'epoch': 1.67}
{'loss': 0.5915, 'learning_rate': 0.0001, 'epoch': 1.7}
{'loss': 0.5971, 'learning_rate': 0.0001, 'epoch': 1.72}
{'eval_loss': 0.7299780249595642, 'eval_runtime': 819.9895, 'eval_samples_per_second': 1.22, 'eval_steps_per_second': 1.22, 'epoch': 1.74}
{'mmlu_loss': 2.720237163840206, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5625, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7674418604651163, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.6666666666666666, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.5483870967741935, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.6363636363636364, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy': 0.6071059477084991, 'epoch': 1.74}
{'loss': 0.5931, 'learning_rate': 0.0001, 'epoch': 1.74}
{'loss': 0.6163, 'learning_rate': 0.0001, 'epoch': 1.77}
{'loss': 0.618, 'learning_rate': 0.0001, 'epoch': 1.79}
{'loss': 0.6216, 'learning_rate': 0.0001, 'epoch': 1.81}
{'loss': 0.5533, 'learning_rate': 0.0001, 'epoch': 1.84}
{'loss': 0.6086, 'learning_rate': 0.0001, 'epoch': 1.86}
Saving PEFT checkpoint...
{'loss': 0.6142, 'learning_rate': 0.0001, 'epoch': 1.88}
{'loss': 0.6232, 'learning_rate': 0.0001, 'epoch': 1.9}
{'loss': 0.6209, 'learning_rate': 0.0001, 'epoch': 1.93}
{'loss': 0.6066, 'learning_rate': 0.0001, 'epoch': 1.95}
{'loss': 0.5988, 'learning_rate': 0.0001, 'epoch': 1.97}
{'loss': 0.6303, 'learning_rate': 0.0001, 'epoch': 2.0}
{'loss': 0.4474, 'learning_rate': 0.0001, 'epoch': 2.02}
{'loss': 0.4471, 'learning_rate': 0.0001, 'epoch': 2.04}
{'loss': 0.4266, 'learning_rate': 0.0001, 'epoch': 2.07}
{'loss': 0.426, 'learning_rate': 0.0001, 'epoch': 2.09}
{'loss': 0.4091, 'learning_rate': 0.0001, 'epoch': 2.11}
{'loss': 0.4365, 'learning_rate': 0.0001, 'epoch': 2.14}
{'loss': 0.4416, 'learning_rate': 0.0001, 'epoch': 2.16}
{'eval_loss': 0.7763813734054565, 'eval_runtime': 819.0009, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 2.17}
{'mmlu_loss': 2.9320408916411096, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6976744186046512, 'mmlu_eval_accuracy_human_sexuality': 0.75, 'mmlu_eval_accuracy_virology': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7790697674418605, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.21428571428571427, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6296296296296297, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_college_medicine': 0.5, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.5769230769230769, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_statistics': 0.5652173913043478, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy': 0.5931937815500581, 'epoch': 2.17}
{'loss': 0.4362, 'learning_rate': 0.0001, 'epoch': 2.18}
{'loss': 0.4168, 'learning_rate': 0.0001, 'epoch': 2.21}
{'loss': 0.4029, 'learning_rate': 0.0001, 'epoch': 2.23}
{'loss': 0.4579, 'learning_rate': 0.0001, 'epoch': 2.25}
{'loss': 0.4409, 'learning_rate': 0.0001, 'epoch': 2.28}
{'loss': 0.4395, 'learning_rate': 0.0001, 'epoch': 2.3}
{'loss': 0.4342, 'learning_rate': 0.0001, 'epoch': 2.32}
Saving PEFT checkpoint...
{'loss': 0.4167, 'learning_rate': 0.0001, 'epoch': 2.35}
{'loss': 0.4197, 'learning_rate': 0.0001, 'epoch': 2.37}
{'loss': 0.4439, 'learning_rate': 0.0001, 'epoch': 2.39}
{'loss': 0.434, 'learning_rate': 0.0001, 'epoch': 2.42}
{'loss': 0.4317, 'learning_rate': 0.0001, 'epoch': 2.44}
{'loss': 0.4051, 'learning_rate': 0.0001, 'epoch': 2.46}
{'loss': 0.4491, 'learning_rate': 0.0001, 'epoch': 2.49}
{'loss': 0.4443, 'learning_rate': 0.0001, 'epoch': 2.51}
{'loss': 0.4572, 'learning_rate': 0.0001, 'epoch': 2.53}
{'loss': 0.434, 'learning_rate': 0.0001, 'epoch': 2.56}
{'loss': 0.4122, 'learning_rate': 0.0001, 'epoch': 2.58}
{'loss': 0.4256, 'learning_rate': 0.0001, 'epoch': 2.6}
{'eval_loss': 0.7761979699134827, 'eval_runtime': 819.001, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 2.61}
{'mmlu_loss': 2.975792651235629, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.7428571428571429, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_astronomy': 0.6875, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5625, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7209302325581395, 'mmlu_eval_accuracy_human_sexuality': 0.75, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_us_history': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3448275862068966, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.4878048780487805, 'mmlu_eval_accuracy_professional_law': 0.4117647058823529, 'mmlu_eval_accuracy_college_medicine': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.5161290322580645, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.5217391304347826, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy': 0.6082965423918562, 'epoch': 2.61}
{'loss': 0.46, 'learning_rate': 0.0001, 'epoch': 2.62}
{'loss': 0.4641, 'learning_rate': 0.0001, 'epoch': 2.65}
{'loss': 0.4564, 'learning_rate': 0.0001, 'epoch': 2.67}
{'loss': 0.4311, 'learning_rate': 0.0001, 'epoch': 2.69}
{'loss': 0.4398, 'learning_rate': 0.0001, 'epoch': 2.72}
{'loss': 0.4678, 'learning_rate': 0.0001, 'epoch': 2.74}
{'loss': 0.4489, 'learning_rate': 0.0001, 'epoch': 2.76}
{'loss': 0.4347, 'learning_rate': 0.0001, 'epoch': 2.79}
Saving PEFT checkpoint...
{'loss': 0.4279, 'learning_rate': 0.0001, 'epoch': 2.81}
{'loss': 0.4515, 'learning_rate': 0.0001, 'epoch': 2.83}
{'loss': 0.4502, 'learning_rate': 0.0001, 'epoch': 2.86}
{'loss': 0.4446, 'learning_rate': 0.0001, 'epoch': 2.88}
{'loss': 0.4592, 'learning_rate': 0.0001, 'epoch': 2.9}
{'loss': 0.4394, 'learning_rate': 0.0001, 'epoch': 2.93}
{'loss': 0.4502, 'learning_rate': 0.0001, 'epoch': 2.95}
{'loss': 0.4793, 'learning_rate': 0.0001, 'epoch': 2.97}
{'loss': 0.4504, 'learning_rate': 0.0001, 'epoch': 3.0}
{'loss': 0.2998, 'learning_rate': 0.0001, 'epoch': 3.02}
{'eval_loss': 0.8203299045562744, 'eval_runtime': 818.7408, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 3.04}
{'mmlu_loss': 3.2078324909073164, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.46875, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6976744186046512, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.5454545454545454, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_law': 0.43529411764705883, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5973837330157781, 'epoch': 3.04}
{'loss': 0.2842, 'learning_rate': 0.0001, 'epoch': 3.04}
{'loss': 0.2644, 'learning_rate': 0.0001, 'epoch': 3.07}
{'loss': 0.2357, 'learning_rate': 0.0001, 'epoch': 3.09}
{'loss': 0.216, 'learning_rate': 0.0001, 'epoch': 3.11}
{'loss': 0.2852, 'learning_rate': 0.0001, 'epoch': 3.14}
{'loss': 0.2829, 'learning_rate': 0.0001, 'epoch': 3.16}
{'loss': 0.2733, 'learning_rate': 0.0001, 'epoch': 3.18}
{'loss': 0.2455, 'learning_rate': 0.0001, 'epoch': 3.21}
{'loss': 0.2232, 'learning_rate': 0.0001, 'epoch': 3.23}
{'loss': 0.2842, 'learning_rate': 0.0001, 'epoch': 3.25}
Saving PEFT checkpoint...
{'loss': 0.2928, 'learning_rate': 0.0001, 'epoch': 3.28}
{'loss': 0.2865, 'learning_rate': 0.0001, 'epoch': 3.3}
{'loss': 0.2556, 'learning_rate': 0.0001, 'epoch': 3.32}
{'loss': 0.2295, 'learning_rate': 0.0001, 'epoch': 3.34}
{'loss': 0.3161, 'learning_rate': 0.0001, 'epoch': 3.37}
{'loss': 0.2986, 'learning_rate': 0.0001, 'epoch': 3.39}
{'loss': 0.2922, 'learning_rate': 0.0001, 'epoch': 3.41}
{'loss': 0.2715, 'learning_rate': 0.0001, 'epoch': 3.44}
{'loss': 0.2447, 'learning_rate': 0.0001, 'epoch': 3.46}
{'eval_loss': 0.8829959630966187, 'eval_runtime': 818.8608, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 3.48}
{'mmlu_loss': 3.0493411824720345, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_professional_psychology': 0.6521739130434783, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.2926829268292683, 'mmlu_eval_accuracy_professional_law': 0.4647058823529412, 'mmlu_eval_accuracy_college_medicine': 0.6363636363636364, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy': 0.6007760524403093, 'epoch': 3.48}
{'loss': 0.3113, 'learning_rate': 0.0001, 'epoch': 3.48}
{'loss': 0.3062, 'learning_rate': 0.0001, 'epoch': 3.51}
{'loss': 0.2881, 'learning_rate': 0.0001, 'epoch': 3.53}
{'loss': 0.2699, 'learning_rate': 0.0001, 'epoch': 3.55}
{'loss': 0.2499, 'learning_rate': 0.0001, 'epoch': 3.58}
{'loss': 0.2888, 'learning_rate': 0.0001, 'epoch': 3.6}
{'loss': 0.3119, 'learning_rate': 0.0001, 'epoch': 3.62}
{'loss': 0.2921, 'learning_rate': 0.0001, 'epoch': 3.65}
{'loss': 0.2687, 'learning_rate': 0.0001, 'epoch': 3.67}
{'loss': 0.2502, 'learning_rate': 0.0001, 'epoch': 3.69}
{'loss': 0.3002, 'learning_rate': 0.0001, 'epoch': 3.72}
Saving PEFT checkpoint...
{'loss': 0.3169, 'learning_rate': 0.0001, 'epoch': 3.74}
{'loss': 0.3049, 'learning_rate': 0.0001, 'epoch': 3.76}
{'loss': 0.2795, 'learning_rate': 0.0001, 'epoch': 3.79}
{'loss': 0.2459, 'learning_rate': 0.0001, 'epoch': 3.81}
{'loss': 0.3113, 'learning_rate': 0.0001, 'epoch': 3.83}
{'loss': 0.3174, 'learning_rate': 0.0001, 'epoch': 3.86}
{'loss': 0.2989, 'learning_rate': 0.0001, 'epoch': 3.88}
{'loss': 0.263, 'learning_rate': 0.0001, 'epoch': 3.9}
{'eval_loss': 0.8808173537254333, 'eval_runtime': 818.8569, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 3.91}
{'mmlu_loss': 3.399379958209456, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.7142857142857143, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7209302325581395, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.6666666666666666, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_professional_law': 0.4235294117647059, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.5, 'mmlu_eval_accuracy': 0.5915168374282129, 'epoch': 3.91}
{'loss': 0.2538, 'learning_rate': 0.0001, 'epoch': 3.93}
{'loss': 0.305, 'learning_rate': 0.0001, 'epoch': 3.95}
{'loss': 0.3147, 'learning_rate': 0.0001, 'epoch': 3.97}
{'loss': 0.2738, 'learning_rate': 0.0001, 'epoch': 4.0}
{'loss': 0.187, 'learning_rate': 0.0001, 'epoch': 4.02}
{'loss': 0.1549, 'learning_rate': 0.0001, 'epoch': 4.04}
{'loss': 0.1426, 'learning_rate': 0.0001, 'epoch': 4.07}
{'loss': 0.1384, 'learning_rate': 0.0001, 'epoch': 4.09}
{'loss': 0.1305, 'learning_rate': 0.0001, 'epoch': 4.11}
{'loss': 0.1721, 'learning_rate': 0.0001, 'epoch': 4.13}
{'loss': 0.1697, 'learning_rate': 0.0001, 'epoch': 4.16}
{'loss': 0.1585, 'learning_rate': 0.0001, 'epoch': 4.18}
Saving PEFT checkpoint...
{'loss': 0.1418, 'learning_rate': 0.0001, 'epoch': 4.2}
{'loss': 0.1417, 'learning_rate': 0.0001, 'epoch': 4.23}
{'loss': 0.1845, 'learning_rate': 0.0001, 'epoch': 4.25}
{'loss': 0.1682, 'learning_rate': 0.0001, 'epoch': 4.27}
{'loss': 0.1638, 'learning_rate': 0.0001, 'epoch': 4.3}
{'loss': 0.149, 'learning_rate': 0.0001, 'epoch': 4.32}
{'loss': 0.1408, 'learning_rate': 0.0001, 'epoch': 4.34}
{'eval_loss': 0.9789459109306335, 'eval_runtime': 819.0257, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 4.34}
{'mmlu_loss': 5.514279878194, 'mmlu_eval_accuracy_econometrics': 0.5, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7209302325581395, 'mmlu_eval_accuracy_human_sexuality': 0.4166666666666667, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7674418604651163, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.6363636363636364, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.34615384615384615, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_professional_law': 0.4294117647058823, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.45161290322580644, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.7222222222222222, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6153846153846154, 'mmlu_eval_accuracy_college_computer_science': 0.2727272727272727, 'mmlu_eval_accuracy_moral_scenarios': 0.48, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.5454545454545454, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_electrical_engineering': 0.5625, 'mmlu_eval_accuracy': 0.5963673968810683, 'epoch': 4.34}
{'loss': 0.1789, 'learning_rate': 0.0001, 'epoch': 4.37}
{'loss': 0.1717, 'learning_rate': 0.0001, 'epoch': 4.39}
{'loss': 0.17, 'learning_rate': 0.0001, 'epoch': 4.41}
{'loss': 0.1519, 'learning_rate': 0.0001, 'epoch': 4.44}
{'loss': 0.1544, 'learning_rate': 0.0001, 'epoch': 4.46}
{'loss': 0.1857, 'learning_rate': 0.0001, 'epoch': 4.48}
{'loss': 0.1778, 'learning_rate': 0.0001, 'epoch': 4.51}
{'loss': 0.1734, 'learning_rate': 0.0001, 'epoch': 4.53}
{'loss': 0.1602, 'learning_rate': 0.0001, 'epoch': 4.55}
{'loss': 0.1525, 'learning_rate': 0.0001, 'epoch': 4.58}
{'loss': 0.183, 'learning_rate': 0.0001, 'epoch': 4.6}
{'loss': 0.1783, 'learning_rate': 0.0001, 'epoch': 4.62}
{'loss': 0.174, 'learning_rate': 0.0001, 'epoch': 4.65}
Saving PEFT checkpoint...
{'loss': 0.1577, 'learning_rate': 0.0001, 'epoch': 4.67}
{'loss': 0.1551, 'learning_rate': 0.0001, 'epoch': 4.69}
{'loss': 0.1979, 'learning_rate': 0.0001, 'epoch': 4.72}
{'loss': 0.177, 'learning_rate': 0.0001, 'epoch': 4.74}
{'loss': 0.1771, 'learning_rate': 0.0001, 'epoch': 4.76}
{'eval_loss': 0.923424482345581, 'eval_runtime': 819.9865, 'eval_samples_per_second': 1.22, 'eval_steps_per_second': 1.22, 'epoch': 4.78}
{'mmlu_loss': 5.392044371362608, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.3793103448275862, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_human_aging': 0.8260869565217391, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.43902439024390244, 'mmlu_eval_accuracy_professional_law': 0.4, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_moral_disputes': 0.6052631578947368, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.36363636363636365, 'mmlu_eval_accuracy_moral_scenarios': 0.46, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5925881909230152, 'epoch': 4.78}
{'loss': 0.1621, 'learning_rate': 0.0001, 'epoch': 4.79}
{'loss': 0.1611, 'learning_rate': 0.0001, 'epoch': 4.81}
{'loss': 0.1819, 'learning_rate': 0.0001, 'epoch': 4.83}
{'loss': 0.1858, 'learning_rate': 0.0001, 'epoch': 4.85}
{'loss': 0.1777, 'learning_rate': 0.0001, 'epoch': 4.88}
{'loss': 0.1572, 'learning_rate': 0.0001, 'epoch': 4.9}
{'loss': 0.1598, 'learning_rate': 0.0001, 'epoch': 4.92}
{'loss': 0.182, 'learning_rate': 0.0001, 'epoch': 4.95}
{'loss': 0.1753, 'learning_rate': 0.0001, 'epoch': 4.97}
{'loss': 0.163, 'learning_rate': 0.0001, 'epoch': 4.99}
{'loss': 0.1153, 'learning_rate': 0.0001, 'epoch': 5.02}
{'loss': 0.0909, 'learning_rate': 0.0001, 'epoch': 5.04}
{'loss': 0.0902, 'learning_rate': 0.0001, 'epoch': 5.06}
{'loss': 0.0987, 'learning_rate': 0.0001, 'epoch': 5.09}
{'loss': 0.1052, 'learning_rate': 0.0001, 'epoch': 5.11}
Saving PEFT checkpoint...
{'loss': 0.1029, 'learning_rate': 0.0001, 'epoch': 5.13}
{'loss': 0.0891, 'learning_rate': 0.0001, 'epoch': 5.16}
{'loss': 0.0998, 'learning_rate': 0.0001, 'epoch': 5.18}
{'loss': 0.1055, 'learning_rate': 0.0001, 'epoch': 5.2}
{'eval_loss': 0.9965302348136902, 'eval_runtime': 818.7499, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 5.21}
{'mmlu_loss': 5.86324098388951, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7441860465116279, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7790697674418605, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.84, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.38461538461538464, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.3170731707317073, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5829852468017358, 'epoch': 5.21}
{'loss': 0.1171, 'learning_rate': 0.0001, 'epoch': 5.23}
{'loss': 0.1142, 'learning_rate': 0.0001, 'epoch': 5.25}
{'loss': 0.0927, 'learning_rate': 0.0001, 'epoch': 5.27}
{'loss': 0.1053, 'learning_rate': 0.0001, 'epoch': 5.3}
{'loss': 0.1004, 'learning_rate': 0.0001, 'epoch': 5.32}
{'loss': 0.1133, 'learning_rate': 0.0001, 'epoch': 5.34}
{'loss': 0.1162, 'learning_rate': 0.0001, 'epoch': 5.37}
{'loss': 0.0995, 'learning_rate': 0.0001, 'epoch': 5.39}
{'loss': 0.1055, 'learning_rate': 0.0001, 'epoch': 5.41}
{'loss': 0.1061, 'learning_rate': 0.0001, 'epoch': 5.44}
{'loss': 0.1127, 'learning_rate': 0.0001, 'epoch': 5.46}
{'loss': 0.117, 'learning_rate': 0.0001, 'epoch': 5.48}
{'loss': 0.1013, 'learning_rate': 0.0001, 'epoch': 5.51}
{'loss': 0.111, 'learning_rate': 0.0001, 'epoch': 5.53}
{'loss': 0.1088, 'learning_rate': 0.0001, 'epoch': 5.55}
{'loss': 0.119, 'learning_rate': 0.0001, 'epoch': 5.57}
Saving PEFT checkpoint...
{'loss': 0.1172, 'learning_rate': 0.0001, 'epoch': 5.6}
{'loss': 0.1035, 'learning_rate': 0.0001, 'epoch': 5.62}
{'loss': 0.1078, 'learning_rate': 0.0001, 'epoch': 5.64}
{'eval_loss': 0.9854245781898499, 'eval_runtime': 819.0277, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 5.65}
{'mmlu_loss': 5.694960174959209, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.6666666666666666, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6976744186046512, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7790697674418605, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8095238095238095, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.6086956521739131, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_elementary_mathematics': 0.2682926829268293, 'mmlu_eval_accuracy_professional_law': 0.3941176470588235, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.5483870967741935, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_college_biology': 0.6875, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5865352241382706, 'epoch': 5.65}
{'loss': 0.1134, 'learning_rate': 0.0001, 'epoch': 5.67}
{'loss': 0.1204, 'learning_rate': 0.0001, 'epoch': 5.69}
{'loss': 0.1179, 'learning_rate': 0.0001, 'epoch': 5.71}
{'loss': 0.1019, 'learning_rate': 0.0001, 'epoch': 5.74}
{'loss': 0.1111, 'learning_rate': 0.0001, 'epoch': 5.76}
{'loss': 0.109, 'learning_rate': 0.0001, 'epoch': 5.78}
{'loss': 0.1213, 'learning_rate': 0.0001, 'epoch': 5.81}
{'loss': 0.1233, 'learning_rate': 0.0001, 'epoch': 5.83}
{'loss': 0.1024, 'learning_rate': 0.0001, 'epoch': 5.85}
{'loss': 0.1124, 'learning_rate': 0.0001, 'epoch': 5.88}
{'loss': 0.1091, 'learning_rate': 0.0001, 'epoch': 5.9}
{'loss': 0.1266, 'learning_rate': 0.0001, 'epoch': 5.92}
{'loss': 0.1237, 'learning_rate': 0.0001, 'epoch': 5.95}
{'loss': 0.1135, 'learning_rate': 0.0001, 'epoch': 5.97}
{'loss': 0.1179, 'learning_rate': 0.0001, 'epoch': 5.99}
{'loss': 0.0967, 'learning_rate': 0.0001, 'epoch': 6.02}
{'loss': 0.0703, 'learning_rate': 0.0001, 'epoch': 6.04}
Saving PEFT checkpoint...
{'loss': 0.0733, 'learning_rate': 0.0001, 'epoch': 6.06}
{'eval_loss': 1.0553635358810425, 'eval_runtime': 818.5553, 'eval_samples_per_second': 1.222, 'eval_steps_per_second': 1.222, 'epoch': 6.08}
{'mmlu_loss': 5.637079143586461, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.7142857142857143, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.45454545454545453, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.11764705882352941, 'mmlu_eval_accuracy_astronomy': 0.625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7674418604651163, 'mmlu_eval_accuracy_human_sexuality': 0.5833333333333334, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7790697674418605, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.3103448275862069, 'mmlu_eval_accuracy_professional_psychology': 0.5507246376811594, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_law': 0.4, 'mmlu_eval_accuracy_college_medicine': 0.5909090909090909, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_moral_disputes': 0.6842105263157895, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_anatomy': 0.5714285714285714, 'mmlu_eval_accuracy_high_school_european_history': 0.6666666666666666, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.48, 'mmlu_eval_accuracy_college_biology': 0.625, 'mmlu_eval_accuracy_high_school_statistics': 0.30434782608695654, 'mmlu_eval_accuracy_us_foreign_policy': 0.8181818181818182, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.25, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5992239203447945, 'epoch': 6.08}
{'loss': 0.0797, 'learning_rate': 0.0001, 'epoch': 6.09}
{'loss': 0.0885, 'learning_rate': 0.0001, 'epoch': 6.11}
{'loss': 0.0801, 'learning_rate': 0.0001, 'epoch': 6.13}
{'loss': 0.0677, 'learning_rate': 0.0001, 'epoch': 6.16}
{'loss': 0.0769, 'learning_rate': 0.0001, 'epoch': 6.18}
{'loss': 0.0825, 'learning_rate': 0.0001, 'epoch': 6.2}
{'loss': 0.0917, 'learning_rate': 0.0001, 'epoch': 6.23}
{'loss': 0.0822, 'learning_rate': 0.0001, 'epoch': 6.25}
{'loss': 0.069, 'learning_rate': 0.0001, 'epoch': 6.27}
{'loss': 0.0764, 'learning_rate': 0.0001, 'epoch': 6.3}
{'loss': 0.0842, 'learning_rate': 0.0001, 'epoch': 6.32}
{'loss': 0.0915, 'learning_rate': 0.0001, 'epoch': 6.34}
{'loss': 0.0824, 'learning_rate': 0.0001, 'epoch': 6.36}
{'loss': 0.07, 'learning_rate': 0.0001, 'epoch': 6.39}
{'loss': 0.0764, 'learning_rate': 0.0001, 'epoch': 6.41}
{'loss': 0.0863, 'learning_rate': 0.0001, 'epoch': 6.43}
{'loss': 0.0969, 'learning_rate': 0.0001, 'epoch': 6.46}
{'loss': 0.08, 'learning_rate': 0.0001, 'epoch': 6.48}
{'loss': 0.0715, 'learning_rate': 0.0001, 'epoch': 6.5}
Saving PEFT checkpoint...
{'eval_loss': 1.090380072593689, 'eval_runtime': 818.2707, 'eval_samples_per_second': 1.222, 'eval_steps_per_second': 1.222, 'epoch': 6.52}
{'mmlu_loss': 5.725950605490875, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.7209302325581395, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7790697674418605, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.4230769230769231, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_college_medicine': 0.6818181818181818, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3548387096774194, 'mmlu_eval_accuracy_moral_disputes': 0.631578947368421, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.4838709677419355, 'mmlu_eval_accuracy_anatomy': 0.5, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_statistics': 0.34782608695652173, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.36363636363636365, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.6025161329757537, 'epoch': 6.52}
{'loss': 0.079, 'learning_rate': 0.0001, 'epoch': 6.53}
{'loss': 0.0844, 'learning_rate': 0.0001, 'epoch': 6.55}
{'loss': 0.0994, 'learning_rate': 0.0001, 'epoch': 6.57}
{'loss': 0.0843, 'learning_rate': 0.0001, 'epoch': 6.6}
{'loss': 0.0698, 'learning_rate': 0.0001, 'epoch': 6.62}
{'loss': 0.0785, 'learning_rate': 0.0001, 'epoch': 6.64}
{'loss': 0.0872, 'learning_rate': 0.0001, 'epoch': 6.67}
{'loss': 0.0975, 'learning_rate': 0.0001, 'epoch': 6.69}
{'loss': 0.0867, 'learning_rate': 0.0001, 'epoch': 6.71}
{'loss': 0.0709, 'learning_rate': 0.0001, 'epoch': 6.74}
{'loss': 0.079, 'learning_rate': 0.0001, 'epoch': 6.76}
{'loss': 0.0849, 'learning_rate': 0.0001, 'epoch': 6.78}
{'loss': 0.0942, 'learning_rate': 0.0001, 'epoch': 6.81}
{'loss': 0.0903, 'learning_rate': 0.0001, 'epoch': 6.83}
{'loss': 0.0739, 'learning_rate': 0.0001, 'epoch': 6.85}
{'loss': 0.0824, 'learning_rate': 0.0001, 'epoch': 6.88}
{'loss': 0.0877, 'learning_rate': 0.0001, 'epoch': 6.9}
{'loss': 0.1003, 'learning_rate': 0.0001, 'epoch': 6.92}
{'loss': 0.0888, 'learning_rate': 0.0001, 'epoch': 6.95}
{'eval_loss': 1.0871186256408691, 'eval_runtime': 829.5317, 'eval_samples_per_second': 1.205, 'eval_steps_per_second': 1.205, 'epoch': 6.95}
{'mmlu_loss': 5.889160635267341, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7558139534883721, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8166666666666667, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.36363636363636365, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_college_medicine': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.4838709677419355, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.4782608695652174, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy': 0.5923879949944959, 'epoch': 6.95}
{'loss': 0.0834, 'learning_rate': 0.0001, 'epoch': 6.97}
Saving PEFT checkpoint...
{'loss': 0.0954, 'learning_rate': 0.0001, 'epoch': 6.99}
{'loss': 0.07, 'learning_rate': 0.0001, 'epoch': 7.02}
{'loss': 0.0519, 'learning_rate': 0.0001, 'epoch': 7.04}
{'loss': 0.0591, 'learning_rate': 0.0001, 'epoch': 7.06}
{'loss': 0.0691, 'learning_rate': 0.0001, 'epoch': 7.08}
{'loss': 0.0778, 'learning_rate': 0.0001, 'epoch': 7.11}
{'loss': 0.064, 'learning_rate': 0.0001, 'epoch': 7.13}
{'loss': 0.0528, 'learning_rate': 0.0001, 'epoch': 7.15}
{'loss': 0.0587, 'learning_rate': 0.0001, 'epoch': 7.18}
{'loss': 0.0687, 'learning_rate': 0.0001, 'epoch': 7.2}
{'loss': 0.0821, 'learning_rate': 0.0001, 'epoch': 7.22}
{'loss': 0.0698, 'learning_rate': 0.0001, 'epoch': 7.25}
{'loss': 0.0558, 'learning_rate': 0.0001, 'epoch': 7.27}
{'loss': 0.0605, 'learning_rate': 0.0001, 'epoch': 7.29}
{'loss': 0.0683, 'learning_rate': 0.0001, 'epoch': 7.32}
{'loss': 0.0826, 'learning_rate': 0.0001, 'epoch': 7.34}
{'loss': 0.0682, 'learning_rate': 0.0001, 'epoch': 7.36}
{'eval_loss': 1.130560278892517, 'eval_runtime': 822.4101, 'eval_samples_per_second': 1.216, 'eval_steps_per_second': 1.216, 'epoch': 7.38}
{'mmlu_loss': 6.154534104756481, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.9090909090909091, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.59375, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.6231884057971014, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_law': 0.4117647058823529, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.2903225806451613, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.4838709677419355, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.45454545454545453, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5933879313950398, 'epoch': 7.38}
{'loss': 0.0551, 'learning_rate': 0.0001, 'epoch': 7.39}
{'loss': 0.0626, 'learning_rate': 0.0001, 'epoch': 7.41}
{'loss': 0.0718, 'learning_rate': 0.0001, 'epoch': 7.43}
Saving PEFT checkpoint...
{'loss': 0.0875, 'learning_rate': 0.0001, 'epoch': 7.46}
{'loss': 0.0696, 'learning_rate': 0.0001, 'epoch': 7.48}
{'loss': 0.0558, 'learning_rate': 0.0001, 'epoch': 7.5}
{'loss': 0.0625, 'learning_rate': 0.0001, 'epoch': 7.53}
{'loss': 0.0714, 'learning_rate': 0.0001, 'epoch': 7.55}
{'loss': 0.0839, 'learning_rate': 0.0001, 'epoch': 7.57}
{'loss': 0.0683, 'learning_rate': 0.0001, 'epoch': 7.6}
{'loss': 0.0568, 'learning_rate': 0.0001, 'epoch': 7.62}
{'loss': 0.0625, 'learning_rate': 0.0001, 'epoch': 7.64}
{'loss': 0.0708, 'learning_rate': 0.0001, 'epoch': 7.67}
{'loss': 0.0841, 'learning_rate': 0.0001, 'epoch': 7.69}
{'loss': 0.0694, 'learning_rate': 0.0001, 'epoch': 7.71}
{'loss': 0.0581, 'learning_rate': 0.0001, 'epoch': 7.74}
{'loss': 0.0638, 'learning_rate': 0.0001, 'epoch': 7.76}
{'loss': 0.0699, 'learning_rate': 0.0001, 'epoch': 7.78}
{'loss': 0.0848, 'learning_rate': 0.0001, 'epoch': 7.8}
{'eval_loss': 1.1300214529037476, 'eval_runtime': 819.3101, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 7.82}
{'mmlu_loss': 6.111602711257087, 'mmlu_eval_accuracy_econometrics': 0.4166666666666667, 'mmlu_eval_accuracy_prehistory': 0.6857142857142857, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.5454545454545454, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.5, 'mmlu_eval_accuracy_business_ethics': 0.5454545454545454, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5555555555555556, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.6376811594202898, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7941176470588235, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.3448275862068966, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_law': 0.40588235294117647, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_moral_disputes': 0.5789473684210527, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.5806451612903226, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6538461538461539, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_us_foreign_policy': 1.0, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy': 0.5965968832636229, 'epoch': 7.82}
{'loss': 0.0703, 'learning_rate': 0.0001, 'epoch': 7.83}
{'loss': 0.0554, 'learning_rate': 0.0001, 'epoch': 7.85}
{'loss': 0.0639, 'learning_rate': 0.0001, 'epoch': 7.87}
{'loss': 0.0731, 'learning_rate': 0.0001, 'epoch': 7.9}
Saving PEFT checkpoint...
{'loss': 0.0866, 'learning_rate': 0.0001, 'epoch': 7.92}
{'loss': 0.0708, 'learning_rate': 0.0001, 'epoch': 7.94}
{'loss': 0.0624, 'learning_rate': 0.0001, 'epoch': 7.97}
{'loss': 0.0795, 'learning_rate': 0.0001, 'epoch': 7.99}
{'loss': 0.0632, 'learning_rate': 0.0001, 'epoch': 8.01}
{'loss': 0.0465, 'learning_rate': 0.0001, 'epoch': 8.04}
{'loss': 0.0529, 'learning_rate': 0.0001, 'epoch': 8.06}
{'loss': 0.058, 'learning_rate': 0.0001, 'epoch': 8.08}
{'loss': 0.0725, 'learning_rate': 0.0001, 'epoch': 8.11}
{'loss': 0.0617, 'learning_rate': 0.0001, 'epoch': 8.13}
{'loss': 0.0456, 'learning_rate': 0.0001, 'epoch': 8.15}
{'loss': 0.0533, 'learning_rate': 0.0001, 'epoch': 8.18}
{'loss': 0.0624, 'learning_rate': 0.0001, 'epoch': 8.2}
{'loss': 0.0727, 'learning_rate': 0.0001, 'epoch': 8.22}
{'loss': 0.0636, 'learning_rate': 0.0001, 'epoch': 8.25}
{'eval_loss': 1.1793287992477417, 'eval_runtime': 820.9053, 'eval_samples_per_second': 1.218, 'eval_steps_per_second': 1.218, 'epoch': 8.25}
{'mmlu_loss': 6.04330149855511, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.36363636363636365, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.59375, 'mmlu_eval_accuracy_sociology': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7093023255813954, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.4482758620689655, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.36585365853658536, 'mmlu_eval_accuracy_professional_law': 0.3941176470588235, 'mmlu_eval_accuracy_college_medicine': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_professional_accounting': 0.45161290322580644, 'mmlu_eval_accuracy_moral_disputes': 0.5263157894736842, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_medicine': 0.41935483870967744, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.7307692307692307, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_statistics': 0.43478260869565216, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.6875, 'mmlu_eval_accuracy': 0.5827654242554993, 'epoch': 8.25}
{'loss': 0.0482, 'learning_rate': 0.0001, 'epoch': 8.27}
{'loss': 0.0553, 'learning_rate': 0.0001, 'epoch': 8.29}
{'loss': 0.0635, 'learning_rate': 0.0001, 'epoch': 8.32}
{'loss': 0.0771, 'learning_rate': 0.0001, 'epoch': 8.34}
{'loss': 0.0625, 'learning_rate': 0.0001, 'epoch': 8.36}
Saving PEFT checkpoint...
{'loss': 0.0495, 'learning_rate': 0.0001, 'epoch': 8.39}
{'loss': 0.0541, 'learning_rate': 0.0001, 'epoch': 8.41}
{'loss': 0.0607, 'learning_rate': 0.0001, 'epoch': 8.43}
{'loss': 0.0772, 'learning_rate': 0.0001, 'epoch': 8.46}
{'loss': 0.061, 'learning_rate': 0.0001, 'epoch': 8.48}
{'loss': 0.0494, 'learning_rate': 0.0001, 'epoch': 8.5}
{'loss': 0.0568, 'learning_rate': 0.0001, 'epoch': 8.52}
{'loss': 0.0646, 'learning_rate': 0.0001, 'epoch': 8.55}
{'loss': 0.0782, 'learning_rate': 0.0001, 'epoch': 8.57}
{'loss': 0.0647, 'learning_rate': 0.0001, 'epoch': 8.59}
{'loss': 0.0493, 'learning_rate': 0.0001, 'epoch': 8.62}
{'loss': 0.0548, 'learning_rate': 0.0001, 'epoch': 8.64}
{'loss': 0.0652, 'learning_rate': 0.0001, 'epoch': 8.66}
{'loss': 0.0773, 'learning_rate': 0.0001, 'epoch': 8.69}
{'eval_loss': 1.1914578676223755, 'eval_runtime': 818.1756, 'eval_samples_per_second': 1.222, 'eval_steps_per_second': 1.222, 'epoch': 8.69}
{'mmlu_loss': 5.984571699843852, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.65625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.36363636363636365, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.627906976744186, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7209302325581395, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.7692307692307693, 'mmlu_eval_accuracy_high_school_psychology': 0.8666666666666667, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.5172413793103449, 'mmlu_eval_accuracy_professional_psychology': 0.5942028985507246, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7058823529411765, 'mmlu_eval_accuracy_conceptual_physics': 0.46153846153846156, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7777777777777778, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.41379310344827586, 'mmlu_eval_accuracy_elementary_mathematics': 0.4146341463414634, 'mmlu_eval_accuracy_professional_law': 0.38235294117647056, 'mmlu_eval_accuracy_college_medicine': 0.4090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_professional_accounting': 0.4838709677419355, 'mmlu_eval_accuracy_moral_disputes': 0.5, 'mmlu_eval_accuracy_global_facts': 0.2, 'mmlu_eval_accuracy_professional_medicine': 0.3870967741935484, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy': 0.5761711645840186, 'epoch': 8.69}
{'loss': 0.0647, 'learning_rate': 0.0001, 'epoch': 8.71}
{'loss': 0.0474, 'learning_rate': 0.0001, 'epoch': 8.73}
{'loss': 0.0542, 'learning_rate': 0.0001, 'epoch': 8.76}
{'loss': 0.0627, 'learning_rate': 0.0001, 'epoch': 8.78}
{'loss': 0.0746, 'learning_rate': 0.0001, 'epoch': 8.8}
{'loss': 0.0643, 'learning_rate': 0.0001, 'epoch': 8.83}
Saving PEFT checkpoint...
{'loss': 0.0499, 'learning_rate': 0.0001, 'epoch': 8.85}
{'loss': 0.0568, 'learning_rate': 0.0001, 'epoch': 8.87}
{'loss': 0.0645, 'learning_rate': 0.0001, 'epoch': 8.9}
{'loss': 0.0792, 'learning_rate': 0.0001, 'epoch': 8.92}
{'loss': 0.0669, 'learning_rate': 0.0001, 'epoch': 8.94}
{'loss': 0.0564, 'learning_rate': 0.0001, 'epoch': 8.97}
{'loss': 0.0715, 'learning_rate': 0.0001, 'epoch': 8.99}
{'loss': 0.0606, 'learning_rate': 0.0001, 'epoch': 9.01}
{'loss': 0.042, 'learning_rate': 0.0001, 'epoch': 9.04}
{'loss': 0.0493, 'learning_rate': 0.0001, 'epoch': 9.06}
{'loss': 0.0586, 'learning_rate': 0.0001, 'epoch': 9.08}
{'loss': 0.0687, 'learning_rate': 0.0001, 'epoch': 9.11}
{'eval_loss': 1.2234318256378174, 'eval_runtime': 819.7003, 'eval_samples_per_second': 1.22, 'eval_steps_per_second': 1.22, 'epoch': 9.12}
{'mmlu_loss': 6.101305809618989, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.6285714285714286, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.17647058823529413, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.7272727272727273, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.2727272727272727, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.6666666666666666, 'mmlu_eval_accuracy_virology': 0.6111111111111112, 'mmlu_eval_accuracy_high_school_us_history': 0.8181818181818182, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.6666666666666666, 'mmlu_eval_accuracy_miscellaneous': 0.7441860465116279, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.5, 'mmlu_eval_accuracy_high_school_mathematics': 0.5172413793103449, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_jurisprudence': 0.5454545454545454, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.782608695652174, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7352941176470589, 'mmlu_eval_accuracy_conceptual_physics': 0.5, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7037037037037037, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.3793103448275862, 'mmlu_eval_accuracy_elementary_mathematics': 0.3902439024390244, 'mmlu_eval_accuracy_professional_law': 0.38823529411764707, 'mmlu_eval_accuracy_college_medicine': 0.4090909090909091, 'mmlu_eval_accuracy_public_relations': 0.5833333333333334, 'mmlu_eval_accuracy_professional_accounting': 0.41935483870967744, 'mmlu_eval_accuracy_moral_disputes': 0.5263157894736842, 'mmlu_eval_accuracy_global_facts': 0.3, 'mmlu_eval_accuracy_professional_medicine': 0.3870967741935484, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.8333333333333334, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.4, 'mmlu_eval_accuracy_college_biology': 0.5, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy': 0.5818219196339234, 'epoch': 9.12}
{'loss': 0.06, 'learning_rate': 0.0001, 'epoch': 9.13}
{'loss': 0.0455, 'learning_rate': 0.0001, 'epoch': 9.15}
{'loss': 0.0513, 'learning_rate': 0.0001, 'epoch': 9.18}
{'loss': 0.0594, 'learning_rate': 0.0001, 'epoch': 9.2}
{'loss': 0.0692, 'learning_rate': 0.0001, 'epoch': 9.22}
{'loss': 0.0604, 'learning_rate': 0.0001, 'epoch': 9.25}
{'loss': 0.0438, 'learning_rate': 0.0001, 'epoch': 9.27}
{'loss': 0.0507, 'learning_rate': 0.0001, 'epoch': 9.29}
Saving PEFT checkpoint...
{'loss': 0.0612, 'learning_rate': 0.0001, 'epoch': 9.31}
{'loss': 0.0685, 'learning_rate': 0.0001, 'epoch': 9.34}
{'loss': 0.0617, 'learning_rate': 0.0001, 'epoch': 9.36}
{'loss': 0.0442, 'learning_rate': 0.0001, 'epoch': 9.38}
{'loss': 0.0517, 'learning_rate': 0.0001, 'epoch': 9.41}
{'loss': 0.0588, 'learning_rate': 0.0001, 'epoch': 9.43}
{'loss': 0.0719, 'learning_rate': 0.0001, 'epoch': 9.45}
{'loss': 0.0616, 'learning_rate': 0.0001, 'epoch': 9.48}
{'loss': 0.0456, 'learning_rate': 0.0001, 'epoch': 9.5}
{'loss': 0.0519, 'learning_rate': 0.0001, 'epoch': 9.52}
{'loss': 0.0599, 'learning_rate': 0.0001, 'epoch': 9.55}
{'eval_loss': 1.2498352527618408, 'eval_runtime': 817.7393, 'eval_samples_per_second': 1.223, 'eval_steps_per_second': 1.223, 'epoch': 9.56}
{'mmlu_loss': 6.205366004144032, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.5714285714285714, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.2727272727272727, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.5625, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.53125, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.3181818181818182, 'mmlu_eval_accuracy_high_school_geography': 0.9090909090909091, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6511627906976745, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_virology': 0.4444444444444444, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.8076923076923077, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.45454545454545453, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.85, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.5652173913043478, 'mmlu_eval_accuracy_jurisprudence': 0.6363636363636364, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.6956521739130435, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_elementary_mathematics': 0.2926829268292683, 'mmlu_eval_accuracy_professional_law': 0.4117647058823529, 'mmlu_eval_accuracy_college_medicine': 0.45454545454545453, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.41935483870967744, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.41, 'mmlu_eval_accuracy_college_biology': 0.5625, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.18181818181818182, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.75, 'mmlu_eval_accuracy': 0.5754574003988651, 'epoch': 9.56}
{'loss': 0.0736, 'learning_rate': 0.0001, 'epoch': 9.57}
{'loss': 0.0606, 'learning_rate': 0.0001, 'epoch': 9.59}
{'loss': 0.0455, 'learning_rate': 0.0001, 'epoch': 9.62}
{'loss': 0.0524, 'learning_rate': 0.0001, 'epoch': 9.64}
{'loss': 0.0613, 'learning_rate': 0.0001, 'epoch': 9.66}
{'loss': 0.073, 'learning_rate': 0.0001, 'epoch': 9.69}
{'loss': 0.0635, 'learning_rate': 0.0001, 'epoch': 9.71}
{'loss': 0.0473, 'learning_rate': 0.0001, 'epoch': 9.73}
{'loss': 0.0543, 'learning_rate': 0.0001, 'epoch': 9.76}
Saving PEFT checkpoint...
{'loss': 0.0607, 'learning_rate': 0.0001, 'epoch': 9.78}
{'loss': 0.0748, 'learning_rate': 0.0001, 'epoch': 9.8}
{'loss': 0.0657, 'learning_rate': 0.0001, 'epoch': 9.83}
{'loss': 0.0489, 'learning_rate': 0.0001, 'epoch': 9.85}
{'loss': 0.0545, 'learning_rate': 0.0001, 'epoch': 9.87}
{'loss': 0.0637, 'learning_rate': 0.0001, 'epoch': 9.9}
{'loss': 0.0765, 'learning_rate': 0.0001, 'epoch': 9.92}
{'loss': 0.0651, 'learning_rate': 0.0001, 'epoch': 9.94}
{'loss': 0.0521, 'learning_rate': 0.0001, 'epoch': 9.97}
{'loss': 0.0662, 'learning_rate': 0.0001, 'epoch': 9.99}
{'eval_loss': 1.2307987213134766, 'eval_runtime': 818.9239, 'eval_samples_per_second': 1.221, 'eval_steps_per_second': 1.221, 'epoch': 9.99}
{'mmlu_loss': 6.245928150777487, 'mmlu_eval_accuracy_econometrics': 0.3333333333333333, 'mmlu_eval_accuracy_prehistory': 0.6571428571428571, 'mmlu_eval_accuracy_medical_genetics': 0.8181818181818182, 'mmlu_eval_accuracy_college_mathematics': 0.09090909090909091, 'mmlu_eval_accuracy_computer_security': 0.45454545454545453, 'mmlu_eval_accuracy_high_school_physics': 0.23529411764705882, 'mmlu_eval_accuracy_astronomy': 0.5, 'mmlu_eval_accuracy_business_ethics': 0.45454545454545453, 'mmlu_eval_accuracy_nutrition': 0.696969696969697, 'mmlu_eval_accuracy_world_religions': 0.8421052631578947, 'mmlu_eval_accuracy_high_school_biology': 0.5625, 'mmlu_eval_accuracy_sociology': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_chemistry': 0.4090909090909091, 'mmlu_eval_accuracy_high_school_geography': 0.8636363636363636, 'mmlu_eval_accuracy_high_school_macroeconomics': 0.6744186046511628, 'mmlu_eval_accuracy_human_sexuality': 0.5, 'mmlu_eval_accuracy_virology': 0.5, 'mmlu_eval_accuracy_high_school_us_history': 0.7727272727272727, 'mmlu_eval_accuracy_high_school_world_history': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_computer_science': 0.5555555555555556, 'mmlu_eval_accuracy_miscellaneous': 0.7325581395348837, 'mmlu_eval_accuracy_high_school_government_and_politics': 0.8571428571428571, 'mmlu_eval_accuracy_college_physics': 0.36363636363636365, 'mmlu_eval_accuracy_international_law': 0.8461538461538461, 'mmlu_eval_accuracy_high_school_psychology': 0.8333333333333334, 'mmlu_eval_accuracy_formal_logic': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_mathematics': 0.41379310344827586, 'mmlu_eval_accuracy_professional_psychology': 0.5797101449275363, 'mmlu_eval_accuracy_jurisprudence': 0.45454545454545453, 'mmlu_eval_accuracy_machine_learning': 0.45454545454545453, 'mmlu_eval_accuracy_human_aging': 0.7391304347826086, 'mmlu_eval_accuracy_marketing': 0.88, 'mmlu_eval_accuracy_philosophy': 0.7647058823529411, 'mmlu_eval_accuracy_conceptual_physics': 0.5384615384615384, 'mmlu_eval_accuracy_logical_fallacies': 0.7222222222222222, 'mmlu_eval_accuracy_security_studies': 0.7407407407407407, 'mmlu_eval_accuracy_management': 0.8181818181818182, 'mmlu_eval_accuracy_clinical_knowledge': 0.4482758620689655, 'mmlu_eval_accuracy_elementary_mathematics': 0.34146341463414637, 'mmlu_eval_accuracy_professional_law': 0.4176470588235294, 'mmlu_eval_accuracy_college_medicine': 0.5454545454545454, 'mmlu_eval_accuracy_public_relations': 0.6666666666666666, 'mmlu_eval_accuracy_professional_accounting': 0.3870967741935484, 'mmlu_eval_accuracy_moral_disputes': 0.5526315789473685, 'mmlu_eval_accuracy_global_facts': 0.4, 'mmlu_eval_accuracy_professional_medicine': 0.5161290322580645, 'mmlu_eval_accuracy_anatomy': 0.42857142857142855, 'mmlu_eval_accuracy_high_school_european_history': 0.7777777777777778, 'mmlu_eval_accuracy_high_school_microeconomics': 0.6923076923076923, 'mmlu_eval_accuracy_college_computer_science': 0.5454545454545454, 'mmlu_eval_accuracy_moral_scenarios': 0.43, 'mmlu_eval_accuracy_college_biology': 0.4375, 'mmlu_eval_accuracy_high_school_statistics': 0.391304347826087, 'mmlu_eval_accuracy_us_foreign_policy': 0.9090909090909091, 'mmlu_eval_accuracy_abstract_algebra': 0.2727272727272727, 'mmlu_eval_accuracy_college_chemistry': 0.125, 'mmlu_eval_accuracy_electrical_engineering': 0.625, 'mmlu_eval_accuracy': 0.5738294328292628, 'epoch': 9.99}
