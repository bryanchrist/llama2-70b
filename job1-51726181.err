  Running command git clone --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-a7dgdbz6/transformers_818f7564d39b4aa49db169c89a15be48
  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-a7dgdbz6/peft_d645b82156584eaca1dc5cc3f7d365e9
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-a7dgdbz6/accelerate_b3727f2583b0410eb36aaa9f9a4a7f11
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so.11.0'), PosixPath('/home/brc4cb/.conda/envs/falcon_40B/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]Downloading shards:  20%|██        | 3/15 [00:00<00:00, 17.95it/s]Downloading shards:  47%|████▋     | 7/15 [00:00<00:00, 24.35it/s]Downloading shards:  73%|███████▎  | 11/15 [00:00<00:00, 27.75it/s]Downloading shards: 100%|██████████| 15/15 [00:00<00:00, 30.75it/s]Downloading shards: 100%|██████████| 15/15 [00:00<00:00, 28.13it/s]
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:30<07:07, 30.54s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:56<05:59, 27.65s/it]Loading checkpoint shards:  20%|██        | 3/15 [01:21<05:18, 26.50s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [01:45<04:43, 25.74s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [02:12<04:19, 25.91s/it]Loading checkpoint shards:  40%|████      | 6/15 [02:40<04:01, 26.80s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [03:12<03:47, 28.48s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [03:40<03:17, 28.25s/it]Loading checkpoint shards:  60%|██████    | 9/15 [04:07<02:47, 27.96s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [04:34<02:17, 27.47s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [05:00<01:48, 27.08s/it]Loading checkpoint shards:  80%|████████  | 12/15 [05:26<01:20, 26.73s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [05:52<00:53, 26.72s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [06:17<00:25, 25.98s/it]Loading checkpoint shards: 100%|██████████| 15/15 [06:18<00:00, 18.53s/it]Loading checkpoint shards: 100%|██████████| 15/15 [06:18<00:00, 25.23s/it]
Downloading (…)neration_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 167/167 [00:00<00:00, 83.0kB/s]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Found cached dataset json (/home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0)
Loading cached split indices for dataset at /home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0/cache-b27e23f99c75a828.arrow and /home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0/cache-66bd7e01f5f3efc6.arrow
Loading cached split indices for dataset at /home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0/cache-bafad0018173d31e.arrow and /home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0/cache-98baf92489e5d6e8.arrow
Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0/cache-5a865bf7c23e9492.arrow
Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/json/default-6525ab776332377d/0.0.0/cache-2bff072e5e290b0a.arrow
Found cached dataset json (/home/brc4cb/.cache/huggingface/datasets/json/default-b9f0808dbc537992/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 29.80it/s]
  0%|          | 0/5000 [00:00<?, ?it/s]/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/utils/checkpoint.py:408: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Traceback (most recent call last):
  File "/gpfs/gpfs0/project/SDS/research/christ_research/Llama 2/llama2-70b/qlora.py", line 817, in <module>
    train()
  File "/gpfs/gpfs0/project/SDS/research/christ_research/Llama 2/llama2-70b/qlora.py", line 779, in train
    train_result = trainer.train()
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/trainer.py", line 1526, in train
    return inner_training_loop(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/trainer.py", line 1796, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/trainer.py", line 2641, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/trainer.py", line 2666, in compute_loss
    outputs = model(**inputs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/utils/operations.py", line 581, in forward
    return model_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/utils/operations.py", line 569, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/peft/peft_model.py", line 922, in forward
    return self.base_model(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 756, in forward
    outputs = self.model(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 636, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 430, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 223, in forward
    outputs = run_function(*args)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 632, in custom_forward
    return module(*inputs, output_attentions, None)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 359, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 261, in forward
    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/peft/tuners/lora.py", line 1145, in forward
    result += output
RuntimeError: The size of tensor a (1024) must match the size of tensor b (8192) at non-singleton dimension 2
  0%|          | 0/5000 [00:01<?, ?it/s]
