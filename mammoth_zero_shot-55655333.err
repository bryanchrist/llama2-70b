WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-o7t6nm4v/peft_c112adfd432840f5b0ce7f6150edf368
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-o7t6nm4v/accelerate_589adaa2aaa14cc28376462bab105ffa
ERROR: Cannot install -r requirements.txt (line 2), -r requirements.txt (line 3), -r requirements.txt (line 6) and huggingface_hub==0.16.4 because these package versions have conflicting dependencies.
ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:05<01:15,  5.36s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:10<01:06,  5.08s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:15<01:00,  5.02s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:20<00:54,  4.94s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:24<00:49,  4.91s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:29<00:43,  4.88s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:34<00:39,  4.88s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:39<00:33,  4.85s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:44<00:29,  4.84s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:48<00:24,  4.83s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:53<00:19,  4.83s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:58<00:14,  4.83s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:03<00:09,  4.82s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:08<00:04,  4.85s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:08<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:08<00:00,  4.58s/it]
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
slurmstepd: error: *** JOB 55655333 ON udc-an34-1 CANCELLED AT 2023-11-28T08:09:48 ***
