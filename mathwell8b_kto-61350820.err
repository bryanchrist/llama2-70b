

==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.5.0

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.5.0


WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-install-hd_im1nd/transformers_770af130e47c4a29b9cfa14732a7cbad
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-install-hd_im1nd/peft_ca14554b0ff4490c9270c4c3250b7e7e
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-hd_im1nd/accelerate_61f9fa1916d340c186b4cc23d6b6e530
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)


==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.5.0

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.5.0


Traceback (most recent call last):
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/utils/import_utils.py", line 1510, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/training_args.py", line 73, in <module>
    from accelerate.state import AcceleratorState, PartialState
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/__init__.py", line 16, in <module>
    from .accelerator import Accelerator
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/accelerator.py", line 35, in <module>
    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/checkpointing.py", line 24, in <module>
    from .utils import (
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/utils/__init__.py", line 182, in <module>
    from .fsdp_utils import load_fsdp_model, load_fsdp_optimizer, merge_fsdp_weights, save_fsdp_model, save_fsdp_optimizer
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/utils/fsdp_utils.py", line 30, in <module>
    import torch.distributed.checkpoint.format_utils as dist_cp_format_utils
ModuleNotFoundError: No module named 'torch.distributed.checkpoint.format_utils'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/import_utils.py", line 176, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/trainer/kto_config.py", line 17, in <module>
    from transformers import TrainingArguments
  File "<frozen importlib._bootstrap>", line 1055, in _handle_fromlist
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/utils/import_utils.py", line 1500, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/utils/import_utils.py", line 1512, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.training_args because of the following error (look up to see its traceback):
No module named 'torch.distributed.checkpoint.format_utils'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/sfs/weka/scratch/brc4cb/llama2-70b/kto.py", line 3, in <module>
    from trl import KTOConfig, KTOTrainer, ModelConfig, get_peft_config, setup_chat_format
  File "<frozen importlib._bootstrap>", line 1055, in _handle_fromlist
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/import_utils.py", line 167, in __getattr__
    value = getattr(module, name)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/import_utils.py", line 166, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/import_utils.py", line 178, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import trl.trainer.kto_config because of the following error (look up to see its traceback):
Failed to import transformers.training_args because of the following error (look up to see its traceback):
No module named 'torch.distributed.checkpoint.format_utils'
