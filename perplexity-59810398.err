

==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.3.0

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.3.0


WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-install-e4i8t8h6/peft_f26ded31c15647e899c81844b4cc1084
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-e4i8t8h6/accelerate_e3c6921630014ad882291a2b0f230d6d
ERROR: Cannot install -r requirements.txt (line 2), -r requirements.txt (line 3), -r requirements.txt (line 6) and huggingface_hub==0.16.4 because these package versions have conflicting dependencies.
ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)
WARNING: Ignoring invalid distribution -riton-nightly (/sfs/qumulo/qhome/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages)


==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 24.3.0

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=24.3.0


/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Downloading shards:   0%|          | 0/15 [00:00<?, ?it/s]Downloading shards:  27%|██▋       | 4/15 [00:00<00:00, 32.41it/s]Downloading shards:  53%|█████▎    | 8/15 [00:00<00:00, 21.28it/s]Downloading shards:  73%|███████▎  | 11/15 [00:00<00:00, 23.21it/s]Downloading shards: 100%|██████████| 15/15 [00:00<00:00, 25.65it/s]Downloading shards: 100%|██████████| 15/15 [00:00<00:00, 24.96it/s]
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:03<00:42,  3.02s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:05<00:36,  2.81s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:08<00:32,  2.75s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:11<00:30,  2.77s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:14<00:28,  2.85s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:17<00:25,  2.88s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:20<00:23,  2.93s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:23<00:20,  2.96s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:25<00:17,  2.89s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:28<00:14,  2.85s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:31<00:11,  2.88s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:34<00:08,  2.91s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:37<00:05,  2.91s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:40<00:02,  2.85s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:40<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:40<00:00,  2.70s/it]
You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')
