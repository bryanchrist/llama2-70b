  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-s0si4jep/peft_a8dac79f42214a959bcc635aa3f7ea5f
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-s0si4jep/accelerate_869cfa17a2ea4bd9a596e649be595eca
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<01:09,  4.94s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:08<00:53,  4.15s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:12<00:46,  3.88s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:15<00:41,  3.80s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:19<00:38,  3.82s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:23<00:34,  3.80s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:27<00:30,  3.81s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:31<00:27,  3.87s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:34<00:22,  3.75s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:38<00:18,  3.68s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:41<00:14,  3.69s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:45<00:11,  3.77s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:49<00:07,  3.74s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:53<00:03,  3.65s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:53<00:00,  2.62s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:53<00:00,  3.55s/it]
WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.
WARNING:datasets.builder:Found cached dataset csv (/home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 489.47it/s]
WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-789e2502999e7416.arrow
WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-789e2502999e7416.arrow
WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-789e2502999e7416.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-db79731672741871.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-4f9937f95ed3fc2e.arrow
Map:   0%|          | 0/59 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
                                                  Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/gpfs/gpfs0/project/SDS/research/christ_research/Llama 2/llama2-70b/ppo.py", line 232, in <module>
    for prompt_tensor in prompt_tensors:
NameError: name 'prompt_tensors' is not defined
