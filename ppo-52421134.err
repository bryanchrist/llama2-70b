  Running command git clone --quiet https://github.com/huggingface/peft.git /tmp/pip-install-8le9hoa7/peft_a05e90a37bc04eb2a9d8c4884ad9881b
  Running command git clone --quiet https://github.com/huggingface/accelerate.git /tmp/pip-install-8le9hoa7/accelerate_cb2cf4b0beeb463e9d20767b927dc80f
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<00:57,  4.13s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:07<00:46,  3.57s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:10<00:40,  3.41s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:13<00:36,  3.31s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:16<00:32,  3.26s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:19<00:28,  3.22s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:23<00:25,  3.22s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:26<00:22,  3.19s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:29<00:19,  3.19s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:32<00:15,  3.18s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:35<00:12,  3.19s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:39<00:09,  3.19s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:42<00:06,  3.19s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:45<00:03,  3.14s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:45<00:00,  2.26s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:45<00:00,  3.03s/it]
WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.
WARNING:datasets.builder:Found cached dataset csv (/home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 560.51it/s]
WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-789e2502999e7416.arrow
WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-789e2502999e7416.arrow
WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-789e2502999e7416.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-db79731672741871.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-4f9937f95ed3fc2e.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/brc4cb/.cache/huggingface/datasets/csv/default-10416fda93fda02b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-499200ed12fcfae2.arrow
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/gpfs/gpfs0/project/SDS/research/christ_research/Llama 2/llama2-70b/ppo.py", line 296, in <module>
    question = ppo_trainer.generate(prompt_tensors, attention_mask=attention_mask, **generation_kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 454, in generate
    response = self.accelerator.unwrap_model(self.model).generate(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 198, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/peft/peft_model.py", line 977, in generate
    outputs = self.base_model.generate(**kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/generation/utils.py", line 1588, in generate
    return self.sample(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/generation/utils.py", line 2642, in sample
    outputs = self(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 806, in forward
    outputs = self.model(
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/brc4cb/.conda/envs/falcon_40B/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 623, in forward
    batch_size, seq_length = input_ids.shape
ValueError: too many values to unpack (expected 2)
